{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "operational-argument",
   "metadata": {},
   "source": [
    "# 2017 Dataset\n",
    "\n",
    "This dataset differs from the 2015 dataset in that the network data is stored as pcap files rather than csv files. The only problem with this dataset is that it contains no attack data. However, it will be useful as a training dataset. Pcap files are much more generalisable than csv files. The problem with the pcap files is that I need to clean the data myself! The pcap files have been converted to flows using Argus - it is these files that will be loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "monthly-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "##local python file holding the paths to the directories I store the log files in\n",
    "from directories_to_use import argus_text_files_dir, getTestingDir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-distribution",
   "metadata": {},
   "source": [
    "### Load data into Dataframe\n",
    "This is largely straightforward, however, the date is stored in the name of the text file rather than the actual records so need to extract that and add that to the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "champion-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda2\\envs\\SWaT\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (20,21,107) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "E:\\ProgramData\\Anaconda2\\envs\\SWaT\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (107) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2682823, 118)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read Argus text files into one single dataframe\n",
    "\"\"\"\n",
    "def readDataIntoDataframe(argus_text_files_dir):\n",
    "    first_time = True\n",
    "    for t_file in os.listdir(argus_text_files_dir):\n",
    "        if first_time:\n",
    "            data2017_df = pd.read_csv(argus_text_files_dir + t_file)\n",
    "            date = removeDateFromName(t_file)\n",
    "            data2017_df[\"StartTime\"] = data2017_df[\"StartTime\"].apply(lambda x : date + x)\n",
    "            data2017_df[\"LastTime\"] = data2017_df[\"LastTime\"].apply(lambda x : date + x)\n",
    "            first_time = False\n",
    "        else:\n",
    "            temp_df = pd.read_csv(argus_text_files_dir + t_file)\n",
    "            date = removeDateFromName(t_file)\n",
    "            temp_df[\"StartTime\"] = temp_df[\"StartTime\"].apply(lambda x : date + x)\n",
    "            temp_df[\"LastTime\"] = temp_df[\"LastTime\"].apply(lambda x : date + x)\n",
    "            data2017_df = pd.concat([data2017_df, temp_df], ignore_index=True)\n",
    "    return data2017_df\n",
    "\n",
    "def removeDateFromName(filename):\n",
    "    year = a.split(\"_\")[-1][0:4]\n",
    "    month = a.split(\"_\")[-1][4:6]\n",
    "    day = a.split(\"_\")[-1][6:8]\n",
    "    full_date = day +\"-\"+ month +\"-\"+ year + \" \"\n",
    "    return full_date\n",
    "\n",
    "data2017_df = readDataIntoDataframe(argus_text_files_dir)\n",
    "print(data2017_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-repair",
   "metadata": {},
   "source": [
    "### Clean data (i.e. remove columns that are unhelpful)\n",
    "Just removing columns that only contain Nans or only contain one unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "waiting-bridal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  (2682823, 118)\n",
      "Shape after:  (2682823, 70)\n",
      "Shape before:  (2682823, 70)\n",
      "Shape after:  (2682823, 59)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loops through all the columns in the dataframe and removes any that only contain nans\n",
    "\"\"\"\n",
    "def removeNanColumns(dataframe):\n",
    "    print(\"Shape before: \", dataframe.shape)\n",
    "    for col in dataframe:\n",
    "        unique_vals = dataframe[col].unique()\n",
    "        if unique_vals.shape[0] == 1:\n",
    "            if np.isnan(unique_vals[0]):\n",
    "                dataframe = dataframe.drop([col], axis=1)\n",
    "    print(\"Shape after: \", dataframe.shape)\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "Remove columns that only contain one unique value.\n",
    "\"\"\"\n",
    "def removeSingleColumns(dataframe):\n",
    "    print(\"Shape before: \", dataframe.shape)\n",
    "    for col in dataframe:\n",
    "        unique_vals = dataframe[col].unique()\n",
    "        if unique_vals.shape[0] == 1:\n",
    "            dataframe = dataframe.drop([col], axis=1)\n",
    "    print(\"Shape after: \", dataframe.shape)\n",
    "    return dataframe\n",
    "\"\"\"\n",
    "Prints the columns in a dataframe and all its unique values\n",
    "\"\"\"\n",
    "def printColumnsAndUniqueVals(dataframe):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    for col in dataframe.columns:\n",
    "        printmd(\"**\" + col + \"**: \" + str(dataframe[col].unique()))\n",
    "        \n",
    "\"\"\"\n",
    "Basically prints in markdown form, can also render HTML\n",
    "\"\"\"\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "data2017_df = removeNanColumns(data2017_df)\n",
    "data2017_df = removeSingleColumns(data2017_df)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "simplified-hello",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StartTime      0.00000\n",
       "LastTime       0.00000\n",
       "Flgs           0.00000\n",
       "Seq            0.00000\n",
       "Dur            0.00000\n",
       "RunTime        0.00000\n",
       "IdleTime       0.00000\n",
       "Mean           0.00000\n",
       "Sum            0.00000\n",
       "Min            0.00000\n",
       "Max            0.00000\n",
       "SrcAddr        0.00000\n",
       "DstAddr        0.00000\n",
       "Proto          0.00000\n",
       "Sport          0.00000\n",
       "Dport          0.00000\n",
       "sTos           0.00000\n",
       "sDSb           0.00000\n",
       "sTtl           0.00000\n",
       "dTtl          31.48648\n",
       "sHops          0.00000\n",
       "dHops         31.48648\n",
       "sIpId          0.00000\n",
       "Cause          0.00000\n",
       "TotPkts        0.00000\n",
       "SrcPkts        0.00000\n",
       "DstPkts        0.00000\n",
       "TotBytes       0.00000\n",
       "SrcBytes       0.00000\n",
       "DstBytes       0.00000\n",
       "Load           0.00000\n",
       "SrcLoad        0.00000\n",
       "DstLoad        0.00000\n",
       "Loss           0.00000\n",
       "SrcLoss        0.00000\n",
       "DstLoss        0.00000\n",
       "pLoss          0.00000\n",
       "SrcGap         0.00000\n",
       "DstGap         0.00000\n",
       "Rate           0.00000\n",
       "SrcRate        0.00000\n",
       "DstRate        0.00000\n",
       "Dir            0.00000\n",
       "State          0.00000\n",
       "TcpRtt         0.00000\n",
       "SynAck         0.00000\n",
       "AckDat         0.00000\n",
       "Offset         0.00000\n",
       "sMeanPktSz     0.00000\n",
       "dMeanPktSz     0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Print columns and the percentage of nans present in each column\"\"\"\n",
    "data2017_mod_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-endorsement",
   "metadata": {},
   "source": [
    "## Calculated changes made to the dataset\n",
    "### Drop Columns\n",
    "- **Inode** is an IP Address, so very difficult to replace. Also >90% of the values were missing - so will drop it.\n",
    "- **TcpOpt** is 99% Nan so will drop that column\n",
    "- **SrcTCPBase** and **DstTCPBase** are 43% missing and they refer to the base sequence number in a TCP transaction. As they are specifically related to TCP, it's not a surprise that there would be values missing since not all the traffic is TCP related. Can't replace with mean as not all traffic is TCP - if I replace with 0, it will skew data. Will have to remove the columns \n",
    "- **SrcWin** and **DstWin** seem to refer to jitter. They are quite useful but with 43% of values being Nans means that it might be misleading to include them. Will have to remove them. \n",
    "- **dTos** and **dDSb** can be removed as their only unique values are 0 and nan.\n",
    "- **sIpId** and **dIpId** are not necessary. It is unique IDs that tie src, dst and port. As they contain nulls and are difficult to simply impute. They can be removed since we have all the individual elements that make up the IDs.\n",
    "- **sVid** and **dVid** can be removed since sVLan and dVLan are the same thing, which is VLAN ID.\n",
    "\n",
    "### Drop Rows\n",
    "- **SPort** and **Dport** can not be safely replaced/imputed without causing inconsistencies. Will remove rows that lack these fields - this makes up 10% of the dataset\n",
    "- **sHops**, **sTtl** and **sDSb** contains only 0.5% (after removing rows from above) of nan values to might as well remove rows containing Nans for this.\n",
    "\n",
    "### Replace with 0\n",
    "- **SrcGap** and **DstGap** is 43% nan but refers to bytes missing from the stream, therefore, when Nan, we can just replace with 0. \n",
    "- **sTos** refers to Type of Service (whether traffic should take precedence etc.). Nans can be set to 0 for these fields.\n",
    "\n",
    "### Imputed\n",
    "- **dHops** will need to be imputed. Values are either 0 or 1.\n",
    "- **dTtl** will also need to be imputed.\n",
    "- **TODO** currently going to remove the columns as they contain 40% Nans. Will attempt to impute them at a later stage and determine if it affects the classification result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "unsigned-prevention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2391347, 48)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Drop columns with too many nulls\n",
    "\"\"\"\n",
    "def dropChosenColumns(dataframe, column_names):\n",
    "    for column in column_names: \n",
    "        dataframe = dataframe.drop([column], axis=1)\n",
    "    return dataframe\n",
    "        \n",
    "\"\"\"\n",
    "Replace Nans with 0\n",
    "\"\"\"\n",
    "def replaceNansWithZero(dataframe, column_names):\n",
    "    for column in column_names:\n",
    "        dataframe[column] = dataframe[column].fillna(0)\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "Remove rows with nans for chosen columns\n",
    "\"\"\"\n",
    "def removeNanRows(dataframe, column_names):\n",
    "    for column in column_names:\n",
    "        dataframe = dataframe[dataframe[column].notna()]\n",
    "    return dataframe\n",
    "\n",
    "data2017_mod_df = data2017_df.copy(deep=True)\n",
    "data2017_mod_df = dropChosenColumns(data2017_mod_df, ['TcpOpt', 'Inode', 'SrcWin', 'DstWin', 'dTos', 'dDSb', 'SrcTCPBase', 'DstTCPBase', 'dIpId', 'dHops', 'dTtl'])\n",
    "data2017_mod_df = removeNanRows(data2017_mod_df, ['Sport', 'Dport', 'sIpId', 'sHops', 'sTtl', 'sDSb'])\n",
    "data2017_mod_df = replaceNansWithZero(data2017_mod_df, ['SrcGap', 'DstGap', 'sTos'])\n",
    "data2017_mod_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-thailand",
   "metadata": {},
   "source": [
    "## Convert all data to numeric form so that it can be passed to ML classifiers\n",
    "- **StartTime** and **LastTime** need to be converted to timestamps - though they will not be included in the classification\n",
    "- **SrcAddr** and **DstAddr** need to be converted to unique values. \n",
    "- **Sport** and **Dport** needs to be cast from string and checked for hex (which would also have to be cast).\n",
    "- **Flgs**, **Dir**, **Cause**, and **State** need to be converted to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "italian-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2391347, 48)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Needs to know the category e.g. IPs and value to add to the dict of dicts (unique_vals)\n",
    "Returns: the unique int assigned to the value\n",
    "\"\"\"\n",
    "def convertToNum(category, val, unique_vals):\n",
    "    if category not in unique_vals:\n",
    "        unique_vals[category] = {}\n",
    "    if val.strip() not in unique_vals[category].keys():\n",
    "        new_val = len(unique_vals[category].keys())\n",
    "        unique_vals[category][val.strip()] = len(unique_vals[category].keys())\n",
    "        return new_val\n",
    "    else:\n",
    "        return unique_vals[category][val.strip()]\n",
    "    \n",
    "\"\"\"\n",
    "Port numbers are a special case. They're integers stored as strings.\n",
    "They are either hex numbers or standard int strings - therefore, we \n",
    "need to check for hex before casting.\n",
    "\"\"\"  \n",
    "def convertPortToNum(val):\n",
    "    if type(val) == str:\n",
    "        isHex = '0x' in val\n",
    "    elif type(val) == float:\n",
    "        isHex = False\n",
    "    if isHex:\n",
    "        return int(val, base=16)\n",
    "    else:\n",
    "        return int(val)\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Takes a string in the following format:\n",
    "14-06-2017 11:25:58.288831\n",
    "Returns: Timestamp\n",
    "\"\"\"\n",
    "def createTimestamp(datetime_string):\n",
    "    row_date = datetime.strptime(datetime_string, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    timestamp = row_date.replace(tzinfo=timezone.utc).timestamp()\n",
    "    return timestamp\n",
    "\n",
    "unique_vals = dict()\n",
    "\n",
    "data2017_mod_df[\"Sport\"] = data2017_mod_df[\"Sport\"].apply(lambda x : convertPortToNum(x))\n",
    "data2017_mod_df[\"Dport\"] = data2017_mod_df[\"Dport\"].apply(lambda x : convertPortToNum(x))\n",
    "data2017_mod_df[\"StartTime\"] = data2017_mod_df[\"StartTime\"].apply(lambda x: createTimestamp(x))\n",
    "data2017_mod_df[\"LastTime\"] = data2017_mod_df[\"LastTime\"].apply(lambda x: createTimestamp(x))\n",
    "data2017_mod_df[\"SrcAddr\"] = data2017_mod_df[\"SrcAddr\"].apply(lambda x : convertToNum(\"ips\", x, unique_vals))\n",
    "data2017_mod_df[\"DstAddr\"] = data2017_mod_df[\"DstAddr\"].apply(lambda x : convertToNum(\"ips\", x, unique_vals))\n",
    "data2017_mod_df[\"Cause\"] = data2017_mod_df[\"Cause\"].apply(lambda x : convertToNum(\"cause\", x, unique_vals))\n",
    "data2017_mod_df[\"State\"] = data2017_mod_df[\"State\"].apply(lambda x : convertToNum(\"state\", x, unique_vals))\n",
    "data2017_mod_df[\"Flgs\"] = data2017_mod_df[\"Flgs\"].apply(lambda x : convertToNum(\"flgs\", x, unique_vals))\n",
    "data2017_mod_df[\"Dir\"] = data2017_mod_df[\"Dir\"].apply(lambda x : convertToNum(\"dir\", x, unique_vals))\n",
    "print(data2017_mod_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-pasta",
   "metadata": {},
   "source": [
    "## Load in malicious data from 2019 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "periodic-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4503102, 118)\n"
     ]
    }
   ],
   "source": [
    "data2019_df = readDataIntoDataframe(getTestingDir())\n",
    "print(data2019_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-oasis",
   "metadata": {},
   "source": [
    "## Reduce Feature set so that both datasets have the same amount of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "wireless-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  (4503102, 118)\n",
      "Shape after:  (4503102, 74)\n",
      "Shape before:  (4503102, 74)\n",
      "Shape after:  (4503102, 63)\n"
     ]
    }
   ],
   "source": [
    "def alignToTrainingData(train_df, test_df):\n",
    "    for col in test_df.columns:\n",
    "        if col not in train_df.columns:\n",
    "            test_df = test_df.drop([col], axis=1)\n",
    "    return test_df\n",
    "\n",
    "data2019_mod_df = data2019_df.copy(deep=True)\n",
    "data2019_mod_df = alignToTrainingData(data2017_mod_df, data2019_mod_df)\n",
    "print(data2019_mod_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-gateway",
   "metadata": {},
   "source": [
    "### Check for any remaning Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "noted-insider",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StartTime     0.0\n",
       "LastTime      0.0\n",
       "Flgs          0.0\n",
       "Seq           0.0\n",
       "Dur           0.0\n",
       "RunTime       0.0\n",
       "IdleTime      0.0\n",
       "Mean          0.0\n",
       "Sum           0.0\n",
       "Min           0.0\n",
       "Max           0.0\n",
       "SrcAddr       0.0\n",
       "DstAddr       0.0\n",
       "Proto         0.0\n",
       "Sport         0.0\n",
       "Dport         0.0\n",
       "sTos          0.0\n",
       "sDSb          0.0\n",
       "sTtl          0.0\n",
       "sHops         0.0\n",
       "sIpId         0.0\n",
       "Cause         0.0\n",
       "TotPkts       0.0\n",
       "SrcPkts       0.0\n",
       "DstPkts       0.0\n",
       "TotBytes      0.0\n",
       "SrcBytes      0.0\n",
       "DstBytes      0.0\n",
       "Load          0.0\n",
       "SrcLoad       0.0\n",
       "DstLoad       0.0\n",
       "Loss          0.0\n",
       "SrcLoss       0.0\n",
       "DstLoss       0.0\n",
       "pLoss         0.0\n",
       "SrcGap        0.0\n",
       "DstGap        0.0\n",
       "Rate          0.0\n",
       "SrcRate       0.0\n",
       "DstRate       0.0\n",
       "Dir           0.0\n",
       "State         0.0\n",
       "TcpRtt        0.0\n",
       "SynAck        0.0\n",
       "AckDat        0.0\n",
       "Offset        0.0\n",
       "sMeanPktSz    0.0\n",
       "dMeanPktSz    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 4000\n",
    "data2019_mod_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-federation",
   "metadata": {},
   "source": [
    "### Remove any rows that contain Nans (provided the percentage of nans is not large <2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "generous-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019_mod_df = removeNanRows(data2019_mod_df, ['SrcGap', 'DstGap', 'Sport', 'Dport', 'sIpId', 'sHops', 'sTtl', 'sDSb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
