{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "operational-argument",
   "metadata": {},
   "source": [
    "# 2017 Dataset\n",
    "\n",
    "This dataset differs from the 2015 dataset in that the network data is stored as pcap files rather than csv files. The only problem with this dataset is that it contains no attack data. However, it will be useful as a training dataset. Pcap files are much more generalisable than csv files. The problem with the pcap files is that I need to clean the data myself! The pcap files have been converted to flows using Argus - it is these files that will be loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "monthly-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import pickle\n",
    "\n",
    "##local python file holding the paths to the directories I store the log files in\n",
    "from directories_to_use import argus_text_files_dir, getTestingDir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-distribution",
   "metadata": {},
   "source": [
    "### Load data into Dataframe\n",
    "This is largely straightforward, however, the date is stored in the name of the text file rather than the actual records so need to extract that and add that to the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "champion-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda2\\envs\\SWaT\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (20,21,107) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "E:\\ProgramData\\Anaconda2\\envs\\SWaT\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (107) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2682823, 118)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read Argus text files into one single dataframe\n",
    "\"\"\"\n",
    "def readDataIntoDataframe(argus_text_files_dir):\n",
    "    first_time = True\n",
    "    for t_file in os.listdir(argus_text_files_dir):\n",
    "        if first_time:\n",
    "            new_df = pd.read_csv(argus_text_files_dir + t_file)\n",
    "            date = removeDateFromName(t_file)\n",
    "            new_df[\"StartTime\"] = new_df[\"StartTime\"].apply(lambda x : date + x)\n",
    "            new_df[\"LastTime\"] = new_df[\"LastTime\"].apply(lambda x : date + x)\n",
    "            first_time = False\n",
    "        else:\n",
    "            temp_df = pd.read_csv(argus_text_files_dir + t_file)\n",
    "            date = removeDateFromName(t_file)\n",
    "            temp_df[\"StartTime\"] = temp_df[\"StartTime\"].apply(lambda x : date + x)\n",
    "            temp_df[\"LastTime\"] = temp_df[\"LastTime\"].apply(lambda x : date + x)\n",
    "            new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def removeDateFromName(filename):\n",
    "    year = filename.split(\"_\")[-1][0:4]\n",
    "    month = filename.split(\"_\")[-1][4:6]\n",
    "    day = filename.split(\"_\")[-1][6:8]\n",
    "    full_date = day +\"-\"+ month +\"-\"+ year + \" \"\n",
    "    return full_date\n",
    "\n",
    "data2017_df = readDataIntoDataframe(argus_text_files_dir)\n",
    "print(data2017_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-repair",
   "metadata": {},
   "source": [
    "### Clean data (i.e. remove columns that are unhelpful)\n",
    "Just removing columns that only contain Nans or only contain one unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "waiting-bridal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  (2682823, 118)\n",
      "Shape after:  (2682823, 70)\n",
      "Columns with only Nans: ['SrcMac', 'DstMac', 'SrcOui', 'DstOui', 'sCo', 'dCo', 'sMpls', 'dMpls', 'sAS', 'dAS', 'iAS', 'NStrok', 'sNStrok', 'dNStrok', 'SIntPkt', 'SIntDist', 'SIntPktAct', 'SIntActDist', 'SIntPktIdl', 'SIntIdlDist', 'DIntPkt', 'DIntDist', 'DIntPktAct', 'DIntActDist', 'DIntPktIdl', 'DIntIdlDist', 'SrcJitter', 'SrcJitAct', 'DstJitter', 'DstJitAct', 'Label', 'srcUdata', 'dstUdata', 'sVlan', 'dVlan', 'sVid', 'dVid', 'sVpri', 'dVpri', 'SRange', 'ERange', 'sPktSz', 'sMaxPktSz', 'dPktSz', 'dMaxPktSz', 'sMinPktSz', 'dMinPktSz', 'dMinPktSz.1']\n",
      "Shape before:  (2682823, 70)\n",
      "Shape after:  (2682823, 59)\n",
      "Columns with only one unique value: ['Trans', 'StdDev', 'AutoId', 'TotAppByte', 'SAppBytes', 'DAppBytes', 'PCRatio', 'Retrans', 'SrcRetra', 'DstRetra', 'pRetran']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loops through all the columns in the dataframe and removes any that only contain nans\n",
    "\"\"\"\n",
    "def removeNanColumns(dataframe):\n",
    "    print(\"Shape before: \", dataframe.shape)\n",
    "    columns_removed = []\n",
    "    for col in dataframe:\n",
    "        unique_vals = dataframe[col].unique()\n",
    "        if unique_vals.shape[0] == 1:\n",
    "            if np.isnan(unique_vals[0]):\n",
    "                dataframe = dataframe.drop([col], axis=1)\n",
    "                columns_removed.append(col)\n",
    "    print(\"Shape after: \", dataframe.shape)\n",
    "    print(\"Columns with only Nans: \" + str(columns_removed))\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "Remove columns that only contain one unique value.\n",
    "\"\"\"\n",
    "def removeSingleColumns(dataframe):\n",
    "    print(\"Shape before: \", dataframe.shape)\n",
    "    columns_removed = []\n",
    "    for col in dataframe:\n",
    "        unique_vals = dataframe[col].unique()\n",
    "        if unique_vals.shape[0] == 1:\n",
    "            dataframe = dataframe.drop([col], axis=1)\n",
    "            columns_removed.append(col)\n",
    "    print(\"Shape after: \", dataframe.shape)\n",
    "    print(\"Columns with only one unique value: \" + str(columns_removed))\n",
    "    return dataframe\n",
    "\"\"\"\n",
    "Prints the columns in a dataframe and all its unique values\n",
    "\"\"\"\n",
    "def printColumnsAndUniqueVals(dataframe):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    for col in dataframe.columns:\n",
    "        printmd(\"**\" + col + \"**: \" + str(dataframe[col].unique()))\n",
    "        \n",
    "\"\"\"\n",
    "Basically prints in markdown form, can also render HTML\n",
    "\"\"\"\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "data2017_df = removeNanColumns(data2017_df)\n",
    "data2017_df = removeSingleColumns(data2017_df)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "embedded-advantage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StartTime      0.000000\n",
       "LastTime       0.000000\n",
       "Flgs           0.000000\n",
       "Seq            0.000000\n",
       "Dur            0.000000\n",
       "RunTime        0.000000\n",
       "IdleTime       0.000000\n",
       "Mean           0.000000\n",
       "Sum            0.000000\n",
       "Min            0.000000\n",
       "Max            0.000000\n",
       "SrcAddr        0.000000\n",
       "DstAddr        0.000000\n",
       "Proto          0.000000\n",
       "Sport         10.424169\n",
       "Dport         10.424169\n",
       "sTos          10.826022\n",
       "dTos          38.928733\n",
       "sDSb          10.826022\n",
       "dDSb          38.928733\n",
       "sTtl          10.826022\n",
       "dTtl          38.928733\n",
       "sHops         10.826022\n",
       "dHops         38.928733\n",
       "sIpId         10.826022\n",
       "dIpId         38.928733\n",
       "Cause          0.000000\n",
       "TotPkts        0.000000\n",
       "SrcPkts        0.000000\n",
       "DstPkts        0.000000\n",
       "TotBytes       0.000000\n",
       "SrcBytes       0.000000\n",
       "DstBytes       0.000000\n",
       "Load           0.000000\n",
       "SrcLoad        0.000000\n",
       "DstLoad        0.000000\n",
       "Loss           0.000000\n",
       "SrcLoss        0.000000\n",
       "DstLoss        0.000000\n",
       "pLoss          0.000000\n",
       "SrcGap        43.607573\n",
       "DstGap        43.607573\n",
       "Rate           0.000000\n",
       "SrcRate        0.000000\n",
       "DstRate        0.000000\n",
       "Dir            0.000000\n",
       "State          0.000000\n",
       "SrcWin        43.607983\n",
       "DstWin        43.637430\n",
       "SrcTCPBase    43.607573\n",
       "DstTCPBase    43.611375\n",
       "TcpRtt         0.000000\n",
       "SynAck         0.000000\n",
       "AckDat         0.000000\n",
       "TcpOpt        95.110635\n",
       "Inode         99.920010\n",
       "Offset         0.000000\n",
       "sMeanPktSz     0.000000\n",
       "dMeanPktSz     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Print columns and the percentage of nans present in each column\"\"\"\n",
    "data2017_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-numbers",
   "metadata": {},
   "source": [
    "## Calculated changes made to the dataset\n",
    "### Drop Columns\n",
    "- **Inode** is an IP Address, so very difficult to replace. Also >90% of the values were missing - so will drop it.\n",
    "- **TcpOpt** is 99% Nan so will drop that column\n",
    "- **SrcTCPBase** and **DstTCPBase** are 43% missing and they refer to the base sequence number in a TCP transaction. As they are specifically related to TCP, it's not a surprise that there would be values missing since not all the traffic is TCP related. Can't replace with mean as not all traffic is TCP - if I replace with 0, it will skew data. Will have to remove the columns \n",
    "- **SrcWin** and **DstWin** seem to refer to jitter. They are quite useful but with 43% of values being Nans means that it might be misleading to include them. Will have to remove them. \n",
    "- **dTos** and **dDSb** can be removed as their only unique values are 0 and nan.\n",
    "- **sIpId** and **dIpId** are not necessary. It is unique IDs that tie src, dst and port. As they contain nulls and are difficult to simply impute. They can be removed since we have all the individual elements that make up the IDs.\n",
    "- **sVid** and **dVid** can be removed since sVLan and dVLan are the same thing, which is VLAN ID.\n",
    "\n",
    "### Drop Rows\n",
    "- **SPort** and **Dport** can not be safely replaced/imputed without causing inconsistencies. Will remove rows that lack these fields - this makes up 10% of the dataset\n",
    "- **sHops**, **sTtl** and **sDSb** contains only 0.5% (after removing rows from above) of nan values to might as well remove rows containing Nans for this.\n",
    "\n",
    "### Replace with 0\n",
    "- **SrcGap** and **DstGap** is 43% nan but refers to bytes missing from the stream, therefore, when Nan, we can just replace with 0. \n",
    "- **sTos** refers to Type of Service (whether traffic should take precedence etc.). Nans can be set to 0 for these fields.\n",
    "\n",
    "### Imputed\n",
    "- **dHops** will need to be imputed. Values are either 0 or 1.\n",
    "- **dTtl** will also need to be imputed.\n",
    "- **TODO** currently going to remove the columns as they contain 40% Nans. Will attempt to impute them at a later stage and determine if it affects the classification result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ordinary-nancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2391347, 48)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Drop columns with too many nulls\n",
    "\"\"\"\n",
    "def dropChosenColumns(dataframe, column_names):\n",
    "    for column in column_names: \n",
    "        dataframe = dataframe.drop([column], axis=1)\n",
    "    return dataframe\n",
    "        \n",
    "\"\"\"\n",
    "Replace Nans with 0\n",
    "\"\"\"\n",
    "def replaceNansWithZero(dataframe, column_names):\n",
    "    for column in column_names:\n",
    "        dataframe[column] = dataframe[column].fillna(0)\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "Remove rows with nans for chosen columns\n",
    "\"\"\"\n",
    "def removeNanRows(dataframe, column_names):\n",
    "    for column in column_names:\n",
    "        dataframe = dataframe[dataframe[column].notna()]\n",
    "    return dataframe\n",
    "\n",
    "data2017_df = dropChosenColumns(data2017_df, ['TcpOpt', 'Inode', 'SrcWin', 'DstWin', 'dTos', 'dDSb', 'SrcTCPBase', 'DstTCPBase', 'dIpId', 'dHops', 'dTtl'])\n",
    "data2017_df = removeNanRows(data2017_df, ['Sport', 'Dport', 'sIpId', 'sHops', 'sTtl', 'sDSb'])\n",
    "data2017_df = replaceNansWithZero(data2017_df, ['SrcGap', 'DstGap', 'sTos'])\n",
    "data2017_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-mason",
   "metadata": {},
   "source": [
    "## Convert all data to numeric form so that it can be passed to ML classifiers\n",
    "- **StartTime** and **LastTime** need to be converted to timestamps - though they will not be included in the classification\n",
    "- **SrcAddr** and **DstAddr** need to be converted to unique values. \n",
    "- **Sport** and **Dport** needs to be cast from string and checked for hex (which would also have to be cast).\n",
    "- **Flgs**, **Dir**, **Cause**, and **State** need to be converted to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "viral-generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2391347, 48)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Needs to know the category e.g. IPs and value to add to the dict of dicts (unique_vals)\n",
    "Returns: the unique int assigned to the value\n",
    "\"\"\"\n",
    "def convertToNum(category, val, unique_vals):\n",
    "    if category not in unique_vals:\n",
    "        unique_vals[category] = {}\n",
    "    if val.strip() not in unique_vals[category].keys():\n",
    "        new_val = len(unique_vals[category].keys())\n",
    "        unique_vals[category][val.strip()] = len(unique_vals[category].keys())\n",
    "        return new_val\n",
    "    else:\n",
    "        return unique_vals[category][val.strip()]\n",
    "    \n",
    "\"\"\"\n",
    "Port numbers are a special case. They're integers stored as strings.\n",
    "They are either hex numbers or standard int strings - therefore, we \n",
    "need to check for hex before casting.\n",
    "\"\"\"  \n",
    "def convertPortToNum(val):\n",
    "    if type(val) == str:\n",
    "        isHex = '0x' in val\n",
    "    elif type(val) == float:\n",
    "        isHex = False\n",
    "    if isHex:\n",
    "        return int(val, base=16)\n",
    "    else:\n",
    "        return int(val)\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Takes a string in the following format:\n",
    "14-06-2017 11:25:58.288831\n",
    "Returns: Timestamp\n",
    "\"\"\"\n",
    "def createTimestamp(datetime_string):\n",
    "    row_date = datetime.strptime(datetime_string, \"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    timestamp = row_date.replace(tzinfo=timezone.utc).timestamp()\n",
    "    return timestamp\n",
    "\n",
    "unique_vals = dict()\n",
    "\n",
    "data2017_df[\"Sport\"] = data2017_df[\"Sport\"].apply(lambda x : convertPortToNum(x))\n",
    "data2017_df[\"Dport\"] = data2017_df[\"Dport\"].apply(lambda x : convertPortToNum(x))\n",
    "data2017_df[\"StartTime\"] = data2017_df[\"StartTime\"].apply(lambda x: createTimestamp(x))\n",
    "data2017_df[\"LastTime\"] = data2017_df[\"LastTime\"].apply(lambda x: createTimestamp(x))\n",
    "data2017_df[\"SrcAddr\"] = data2017_df[\"SrcAddr\"].apply(lambda x : convertToNum(\"ips\", x, unique_vals))\n",
    "data2017_df[\"DstAddr\"] = data2017_df[\"DstAddr\"].apply(lambda x : convertToNum(\"ips\", x, unique_vals))\n",
    "data2017_df[\"Cause\"] = data2017_df[\"Cause\"].apply(lambda x : convertToNum(\"cause\", x, unique_vals))\n",
    "data2017_df[\"State\"] = data2017_df[\"State\"].apply(lambda x : convertToNum(\"state\", x, unique_vals))\n",
    "data2017_df[\"Flgs\"] = data2017_df[\"Flgs\"].apply(lambda x : convertToNum(\"flgs\", x, unique_vals))\n",
    "data2017_df[\"Dir\"] = data2017_df[\"Dir\"].apply(lambda x : convertToNum(\"dir\", x, unique_vals))\n",
    "print(data2017_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-lewis",
   "metadata": {},
   "source": [
    "## Load in malicious data from 2019 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eastern-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4503102, 118)\n"
     ]
    }
   ],
   "source": [
    "data2019_df = readDataIntoDataframe(getTestingDir())\n",
    "print(data2019_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-testing",
   "metadata": {},
   "source": [
    "## Reduce Feature set so that both datasets have the same amount of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cognitive-server",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4503102, 48)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Takes in two dataframes and makes sure that the columns\n",
    "of test_df are the same as those of train_df. Returns a\n",
    "modified dataframe.\n",
    "\"\"\"\n",
    "def alignToTrainingData(train_df, test_df):\n",
    "    for col in test_df.columns:\n",
    "        if col not in train_df.columns:\n",
    "            test_df = test_df.drop([col], axis=1)\n",
    "    return test_df\n",
    "\n",
    "#data2019_mod_df = data2019_df.copy(deep=True)\n",
    "data2019_df = alignToTrainingData(data2017_df, data2019_df)\n",
    "print(data2019_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-history",
   "metadata": {},
   "source": [
    "### Check for any remaning Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foreign-feedback",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StartTime     0.000000\n",
       "LastTime      0.000000\n",
       "Flgs          0.000000\n",
       "Seq           0.000000\n",
       "Dur           0.000000\n",
       "RunTime       0.000000\n",
       "IdleTime      0.000000\n",
       "Mean          0.000000\n",
       "Sum           0.000000\n",
       "Min           0.000000\n",
       "Max           0.000000\n",
       "SrcAddr       0.000000\n",
       "DstAddr       0.000000\n",
       "Proto         0.000000\n",
       "Sport         0.529946\n",
       "Dport         0.529946\n",
       "sTos          0.438520\n",
       "sDSb          0.438520\n",
       "sTtl          0.438520\n",
       "sHops         0.438520\n",
       "sIpId         0.438520\n",
       "Cause         0.000000\n",
       "TotPkts       0.000000\n",
       "SrcPkts       0.000000\n",
       "DstPkts       0.000000\n",
       "TotBytes      0.000000\n",
       "SrcBytes      0.000000\n",
       "DstBytes      0.000000\n",
       "Load          0.000000\n",
       "SrcLoad       0.000000\n",
       "DstLoad       0.000000\n",
       "Loss          0.000000\n",
       "SrcLoss       0.000000\n",
       "DstLoss       0.000000\n",
       "pLoss         0.000000\n",
       "SrcGap        1.956163\n",
       "DstGap        1.956163\n",
       "Rate          0.000000\n",
       "SrcRate       0.000000\n",
       "DstRate       0.000000\n",
       "Dir           0.000000\n",
       "State         0.000000\n",
       "TcpRtt        0.000000\n",
       "SynAck        0.000000\n",
       "AckDat        0.000000\n",
       "Offset        0.000000\n",
       "sMeanPktSz    0.000000\n",
       "dMeanPktSz    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.options.display.max_rows = 4000\n",
    "data2019_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-tract",
   "metadata": {},
   "source": [
    "### Remove any rows that contain Nans (provided the percentage of nans is not large <2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "second-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019_df = removeNanRows(data2019_df, ['SrcGap', 'DstGap', 'Sport', 'Dport', 'sIpId', 'sHops', 'sTtl', 'sDSb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-genesis",
   "metadata": {},
   "source": [
    "### Convert all non-numeric fields to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "armed-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019_df[\"Sport\"] = data2019_df[\"Sport\"].apply(lambda x : convertPortToNum(x))\n",
    "data2019_df[\"Dport\"] = data2019_df[\"Dport\"].apply(lambda x : convertPortToNum(x))\n",
    "data2019_df[\"StartTime\"] = data2019_df[\"StartTime\"].apply(lambda x: createTimestamp(x))\n",
    "data2019_df[\"LastTime\"] = data2019_df[\"LastTime\"].apply(lambda x: createTimestamp(x))\n",
    "data2019_df[\"SrcAddr\"] = data2019_df[\"SrcAddr\"].apply(lambda x : convertToNum(\"ips\", x, unique_vals))\n",
    "data2019_df[\"DstAddr\"] = data2019_df[\"DstAddr\"].apply(lambda x : convertToNum(\"ips\", x, unique_vals))\n",
    "data2019_df[\"Cause\"] = data2019_df[\"Cause\"].apply(lambda x : convertToNum(\"cause\", x, unique_vals))\n",
    "data2019_df[\"State\"] = data2019_df[\"State\"].apply(lambda x : convertToNum(\"state\", x, unique_vals))\n",
    "data2019_df[\"Flgs\"] = data2019_df[\"Flgs\"].apply(lambda x : convertToNum(\"flgs\", x, unique_vals))\n",
    "data2019_df[\"Dir\"] = data2019_df[\"Dir\"].apply(lambda x : convertToNum(\"dir\", x, unique_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-discovery",
   "metadata": {},
   "source": [
    "### Add a new column representing whether the traffic is malicious or benign. \n",
    "0 is benign and 1 is malicious <br>\n",
    "Go through time period of attack and set the value to 1. <br>\n",
    "**Note** the time period was adjusted due to different time zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apart-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019_df[\"Classification\"] = 0\n",
    "data2017_df[\"Classification\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reasonable-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_attack = (data2019_df[\"StartTime\"] >= createTimestamp(\"06-12-2019 02:20:00.00\")) & (data2019_df[\"StartTime\"] < createTimestamp(\"06-12-2019 03:30:00.00\"))\n",
    "data2019_df.loc[first_attack, \"Classification\"] = 1\n",
    "\n",
    "second_attack = (data2019_df[\"StartTime\"] >= createTimestamp(\"06-12-2019 04:30:00.00\")) & (data2019_df[\"StartTime\"] <= createTimestamp(\"06-12-2019 05:45:00.00\"))\n",
    "data2019_df.loc[second_attack, \"Classification\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-drawing",
   "metadata": {},
   "source": [
    "### Convert to numpy arrays and save\n",
    "This will allow us to actually only load the arrays and unloading everything else (should help with memory issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "presidential-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2019_arr = data2019_df.to_numpy()\n",
    "data2017_arr = data2019_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "after-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data2019_arr.npy', data2019_arr)\n",
    "np.save('data2017_arr.npy', data2017_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "virgin-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is taken straight from: https://stackoverflow.com/questions/19201290/how-to-save-a-dictionary-to-a-file/32216025\n",
    "\"\"\"\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "entire-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(unique_vals, \"unique_vals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
